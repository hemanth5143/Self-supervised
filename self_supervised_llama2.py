# -*- coding: utf-8 -*-
"""self_Supervised_LLAMA2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dka7hJ8aiLAv_6quDcxQCiCb-imApTSL
"""

pip install transformers datasets peft accelerate bitsandbytes trl torch sentencepiece huggingface_hub

pip install PyPDF2

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from datasets import Dataset
from peft import LoraConfig, get_peft_model
from huggingface_hub import login
import PyPDF2

# Step 1: Extract text from PDF
def extract_text_from_pdf(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
    return text

pdf_path = "/content/bhagavad-gita_tal_como_es-spanish-1975_edition.pdf"
corpus = extract_text_from_pdf(pdf_path)

dataset = Dataset.from_dict({"text": [corpus]})

# Assuming your dataset is named 'dataset'
print(dataset[0])

from huggingface_hub import login

token = "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"
login(token)

model_name = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', load_in_8bit=True)

tokenizer.pad_token = tokenizer.eos_token

# Step 5: Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])

print("Tokenized dataset size:", len(tokenized_dataset))
print("Tokenized dataset features:", tokenized_dataset.features)

# Step 6: Configure LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
)

model = get_peft_model(model, lora_config)

# Step 7: Set up training arguments
training_args = TrainingArguments(
    output_dir="./results",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=True,
    learning_rate=2e-5,
    weight_decay=0.01,
    fp16=True,
    logging_steps=500,
    evaluation_strategy="steps",
    eval_steps=5000,
)

# Step 8: Prepare data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Step 9: Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

# Step 10: Train the model
trainer.train()

# Step 11: Save the fine-tuned model
trainer.save_model("./finetuned-llama2-bhagavad-gita")

print("Training complete. Model saved at: ./finetuned-llama2-bhagavad-gita")






